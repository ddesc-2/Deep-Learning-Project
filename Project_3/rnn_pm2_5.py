# -*- coding: utf-8 -*-
"""RNN_pm2.5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10cY1iTQHRzs_rgvQZtOP0u9UYi8bXFCr
"""

import pandas as pd
from pandas import DataFrame
from pandas import concat
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import TimeDistributed
from keras.layers import Activation
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

import csv
from keras.models import load_model

df = pd.read_csv('data.csv',quoting=csv.QUOTE_NONE )
tdf = pd.read_csv('test.csv' ,quoting=csv.QUOTE_NONE)


def data_preprocessing(df):

  Drop_c = ['"""sitename"""','"""county"""','"""pollutant"""','"""status"""','"""unit"""', '"""co_8hr"""',
          '"""so2_avg"""', '"""longitude"""', '"""latitude"""','"""no2"""',
          '"""so2"""','"""winddirec"""','"""co"""','"""o3_8hr"""','"""no"""','"""windspeed"""','"""datacreationdate"""']
  data = df.drop(Drop_c ,axis =1)
  k = 0
  for i in data.values:#'-' , '奇怪的字'為空
    idx = 0
    for j in i:
      if(j == '-' or j== '"""o3_8hr"""' or j== '"""pm2.5_avg"""' or j =='"""pm2.5"""'or j =='"""pm10"""'
        or j =='"""aqi"""' or j == '"""nox"""' or j == '"""o3"""' or j =='"""pm10_avg"""'):
        data.iloc[k,idx] = np.nan
      idx+=1
    k+=1
  for i in data.columns:
    data[i].fillna(value=data[i].mean(), inplace=True)
  return data

train = data_preprocessing(df)
test = data_preprocessing(tdf)
print(train.info())
print(train['"""siteid"""'])
train.dropna(inplace = True)
j = 0
for i in train['"""siteid"""']:
  if i != 64:
    train['"""siteid"""'][j] = np.nan
  j+=1
print(train['"""siteid"""'])
train.dropna(inplace = True)
print(train.info())
print(test.info())

data_Y = train['"""pm2.5"""']
test_Y = test['"""pm2.5"""']



train_dv = train.values #資料
test_dv = test.values
train_Y_dv = data_Y.values #Y
test_Y_dv = test_Y.values
#train_Y_dv = np.asarray(train_Y_dv)
test_Y_dv = np.asarray(test_Y_dv)


train_Y_dv = train_Y_dv.reshape(-1,1)
test_Y_dv = test_Y.values.reshape(-1,1)
s1 = MinMaxScaler(feature_range=(0, 1))
s2 = MinMaxScaler(feature_range=(0, 1))
s3 = MinMaxScaler(feature_range=(0, 1))
s4 = MinMaxScaler(feature_range=(0, 1))
train_set = s1.fit_transform(train_dv)
train_Y = s2.fit_transform(train_Y_dv)
test_set = s3.fit_transform(test_dv)
test_Y = s4.fit_transform(test_Y_dv)

def test_to_supervised(train):#做series
  window_size = 6
  X = []
  Y = []
  temp = 0
  for i in range(window_size, len(train)+1):
    X.append(train[i-window_size:i,:])
  return X,Y

def to_supervised(train):
  window_size = 6
  X = []
  Y = []
  for i in range(window_size, len(train)-6):
    X.append(train[i-window_size:i,:])
    Y.append(train_Y[i:i+6])
  return X,Y

X_test , Y_test = test_to_supervised(test_set)
print(Y_test)
X_test = np.asarray(X_test)
X_test = X_test.astype(np.float32)
Y_test = np.asarray(Y_test)
Y_test = Y_test.astype(np.float32)

X,Y= to_supervised(train_set)
X = np.asarray(X)
X = X.astype(np.float32)
Y = np.asarray(Y)
Y = Y.astype(np.float32)

n_train = 7000 #分割驗證集
X_val, X_train = X[n_train:,] , X[:n_train,]
print('X_train' ,X_train.shape)
print('X_val' ,X_val.shape)
Y_val, Y_train = Y[n_train:,] , Y[:n_train,]
print('Y_train' ,Y_train.shape)
print('Y_val' ,Y_val.shape)
print('X_test' ,X_test.shape)


def reshape_array(arr): #調整Y的矩陣合併一下
  temp = []
  for i in range(len(arr)):
    x = arr[i].reshape((1,6))  
    temp.append(x[0])
  temp = np.asarray(temp)
  temp = temp.astype(np.float32)
  return temp

Y_train = reshape_array(Y_train)
Y_val = reshape_array(Y_val)

from keras.callbacks import EarlyStopping
from tensorflow.python import rnn_cell

RNN = Sequential() #model
RNN.add(LSTM(units = 32, return_sequences = True, input_shape=(X_train.shape[1], X_train.shape[2])))
RNN.add(Dropout(0.5))
RNN.add(LSTM(units = 32, return_sequences = True))
RNN.add(Dropout(0.5))
RNN.add(LSTM(units = 32))
RNN.add((Dense(6)))

early_stopping = EarlyStopping(monitor = 'val_loss' ,patience = 5)

RNN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss = 'mean_squared_error') #訓練模型

history = RNN.fit(X_train, Y_train, epochs = 50, batch_size = 64,validation_data =[X_val,Y_val] ,callbacks =[early_stopping])

RNN.save('new_model.h5')
plt.plot(history.history['loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()



Y_pred = RNN.predict(X_test)
Y_pred = s4.inverse_transform(Y_pred)
Y_pred = np.round(Y_pred)
print(len(Y_pred))
for i in range(96):
  print(Y_pred[i] )

sub2 = pd.read_csv('Sample_Submission.csv' ,quoting=csv.QUOTE_NONE)

for i in range(96):
  sub2['Predict_PM2.5'][i] = str(Y_pred[i])

print(sub2)
sub2.to_csv('Sample_Submission.csv')
